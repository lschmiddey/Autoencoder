{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "9139ca13fc640d8623238ac4ed44beace8a76f86a07bab6efe75c2506e18783d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlprepare as mlp \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/creditcard.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 327
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_base.columns"
   ]
  },
  {
   "source": [
    "We need to normalize Time and Amount"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_time=df_base['Time'].mean()\n",
    "mean_amount=df_base['Amount'].mean()\n",
    "std_time=df_base['Time'].std()\n",
    "std_amount=df_base['Amount'].std()\n",
    "\n",
    "df_base['Time']=(df_base['Time']-mean_time)/std_time\n",
    "df_base['Amount']=(df_base['Amount']-mean_amount)/std_amount"
   ]
  },
  {
   "source": [
    "Class=1 means that this was indeed a fraud case, class=0 means no fraud. This dataset is highly imbalanced:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 185
    }
   ],
   "source": [
    "df_base['Class'].value_counts()"
   ]
  },
  {
   "source": [
    "I want to create fake data based on the 492 cases, which I will then use to improve the model. Let's first train a simple RandomForest."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = mlp.split_df(df_base, dep_var='Class', test_size=0.3, split_mode='random')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    85286\n",
       "1      157\n",
       "Name: Class, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 368
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "543.2229299363057"
      ]
     },
     "metadata": {},
     "execution_count": 369
    }
   ],
   "source": [
    "#Ratio of the two classes:\n",
    "y_test.value_counts()[0]/y_test.value_counts()[1]"
   ]
  },
  {
   "source": [
    "RandomForest with Oversampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(xs, y, n_estimators=40, max_samples=500,\n",
    "       max_features=0.5, min_samples_leaf=5, **kwargs):\n",
    "    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n",
    "        max_samples=max_samples, max_features=max_features,\n",
    "        min_samples_leaf=min_samples_leaf, oob_score=True,  class_weight={0:1,1:543}).fit(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = rf(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[85278,     8],\n",
       "       [  118,    39]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 408
    }
   ],
   "source": [
    "confusion_matrix(y_test, np.round(m.predict(X_test)))"
   ]
  },
  {
   "source": [
    "With this technique we get about 39 out of 157 Fraud cases, although the results vary quite a lot!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Fake Data with VAE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We want only where y_train/test_train =1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fraud = X_train.iloc[np.where(y_train==1)[0]]\n",
    "X_test_fraud = X_test.iloc[np.where(y_test==1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DataBuilder(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.x = dataset.values\n",
    "        self.x = torch.from_numpy(self.x).to(torch.float)\n",
    "        self.len=self.x.shape[0]\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_set=DataBuilder(X_train_fraud)\n",
    "testdata_set=DataBuilder(X_test_fraud)\n",
    "\n",
    "trainloader=DataLoader(dataset=traindata_set,batch_size=1024)\n",
    "testloader=DataLoader(dataset=testdata_set,batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self,D_in,H=50,H2=12,latent_dim=3):\n",
    "        \n",
    "        #Encoder\n",
    "        super(Autoencoder,self).__init__()\n",
    "        self.linear1=nn.Linear(D_in,H)\n",
    "        self.lin_bn1 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear2=nn.Linear(H,H2)\n",
    "        self.lin_bn2 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear3=nn.Linear(H2,H2)\n",
    "        self.lin_bn3 = nn.BatchNorm1d(num_features=H2)\n",
    "        \n",
    "        # Latent vectors mu and sigma\n",
    "        self.fc1 = nn.Linear(H2, latent_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=latent_dim)\n",
    "        self.fc21 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "        # Sampling vector\n",
    "        self.fc3 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc_bn3 = nn.BatchNorm1d(latent_dim)\n",
    "        self.fc4 = nn.Linear(latent_dim, H2)\n",
    "        self.fc_bn4 = nn.BatchNorm1d(H2)\n",
    "        \n",
    "        # Decoder\n",
    "        self.linear4=nn.Linear(H2,H2)\n",
    "        self.lin_bn4 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear5=nn.Linear(H2,H)\n",
    "        self.lin_bn5 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear6=nn.Linear(H,D_in)\n",
    "        self.lin_bn6 = nn.BatchNorm1d(num_features=D_in)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def encode(self, x):\n",
    "        lin1 = self.relu(self.lin_bn1(self.linear1(x)))\n",
    "        lin2 = self.relu(self.lin_bn2(self.linear2(lin1)))\n",
    "        lin3 = self.relu(self.lin_bn3(self.linear3(lin2)))\n",
    "\n",
    "        fc1 = F.relu(self.bn1(self.fc1(lin3)))\n",
    "\n",
    "        r1 = self.fc21(fc1)\n",
    "        r2 = self.fc22(fc1)\n",
    "        \n",
    "        return r1, r2\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def decode(self, z):\n",
    "        fc3 = self.relu(self.fc_bn3(self.fc3(z)))\n",
    "        fc4 = self.relu(self.fc_bn4(self.fc4(fc3)))\n",
    "\n",
    "        lin4 = self.relu(self.lin_bn4(self.linear4(fc4)))\n",
    "        lin5 = self.relu(self.lin_bn5(self.linear5(lin4)))\n",
    "        return self.lin_bn6(self.linear6(lin5))\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(customLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n",
    "    \n",
    "    # x_recon ist der im forward im Model erstellte recon_batch, x ist der originale x Batch, mu ist mu und logvar ist logvar \n",
    "    def forward(self, x_recon, x, mu, logvar):\n",
    "        loss_MSE = self.mse_loss(x_recon, x)\n",
    "        loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        return loss_MSE + loss_KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = traindata_set.x.shape[1]\n",
    "H = 50\n",
    "H2 = 12\n",
    "model = Autoencoder(D_in, H, H2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mse = customLoss()"
   ]
  },
  {
   "source": [
    "## Train Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 50\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(trainloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_mse(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    if epoch % 200 == 0:        \n",
    "        print('====> Epoch: {} Average training loss: {:.4f}'.format(\n",
    "            epoch, train_loss / len(trainloader.dataset)))\n",
    "        train_losses.append(train_loss / len(trainloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        for batch_idx, data in enumerate(testloader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_mse(recon_batch, data, mu, logvar)\n",
    "            test_loss += loss.item()\n",
    "            if epoch % 200 == 0:        \n",
    "                print('====> Epoch: {} Average test loss: {:.4f}'.format(\n",
    "                    epoch, test_loss / len(testloader.dataset)))\n",
    "            test_losses.append(test_loss / len(testloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====> Epoch: 200 Average training loss: 706.2121\n",
      "====> Epoch: 200 Average test loss: 590.0016\n",
      "====> Epoch: 400 Average training loss: 620.5279\n",
      "====> Epoch: 400 Average test loss: 521.3142\n",
      "====> Epoch: 600 Average training loss: 566.4392\n",
      "====> Epoch: 600 Average test loss: 477.5008\n",
      "====> Epoch: 800 Average training loss: 521.7474\n",
      "====> Epoch: 800 Average test loss: 440.3243\n",
      "====> Epoch: 1000 Average training loss: 481.2092\n",
      "====> Epoch: 1000 Average test loss: 407.7625\n",
      "====> Epoch: 1200 Average training loss: 434.3898\n",
      "====> Epoch: 1200 Average test loss: 362.2760\n",
      "====> Epoch: 1400 Average training loss: 396.9551\n",
      "====> Epoch: 1400 Average test loss: 343.7408\n"
     ]
    }
   ],
   "source": [
    "epochs = 1500\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "source": [
    "We're still improving so keep going "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====> Epoch: 200 Average training loss: 343.3472\n",
      "====> Epoch: 200 Average test loss: 300.3575\n",
      "====> Epoch: 400 Average training loss: 310.5800\n",
      "====> Epoch: 400 Average test loss: 285.6697\n",
      "====> Epoch: 600 Average training loss: 281.8408\n",
      "====> Epoch: 600 Average test loss: 263.7150\n",
      "====> Epoch: 800 Average training loss: 256.1950\n",
      "====> Epoch: 800 Average test loss: 244.9427\n",
      "====> Epoch: 1000 Average training loss: 232.6077\n",
      "====> Epoch: 1000 Average test loss: 236.3014\n",
      "====> Epoch: 1200 Average training loss: 211.2899\n",
      "====> Epoch: 1200 Average test loss: 217.6404\n",
      "====> Epoch: 1400 Average training loss: 191.3525\n",
      "====> Epoch: 1400 Average test loss: 205.8287\n",
      "====> Epoch: 1600 Average training loss: 174.0826\n",
      "====> Epoch: 1600 Average test loss: 189.0589\n",
      "====> Epoch: 1800 Average training loss: 157.4292\n",
      "====> Epoch: 1800 Average test loss: 175.6006\n",
      "====> Epoch: 2000 Average training loss: 143.2475\n",
      "====> Epoch: 2000 Average test loss: 177.1668\n",
      "====> Epoch: 2200 Average training loss: 129.9684\n",
      "====> Epoch: 2200 Average test loss: 160.4641\n",
      "====> Epoch: 2400 Average training loss: 117.6745\n",
      "====> Epoch: 2400 Average test loss: 150.9483\n"
     ]
    }
   ],
   "source": [
    "epochs = 2500\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====> Epoch: 200 Average training loss: 54.6816\n",
      "====> Epoch: 200 Average test loss: 129.6853\n",
      "====> Epoch: 400 Average training loss: 48.5159\n",
      "====> Epoch: 400 Average test loss: 134.4429\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "source": [
    "Let's look at the results:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(testloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_row = recon_batch[0].cpu().numpy()\n",
    "recon_row = np.append(recon_row, [1])\n",
    "real_row = testloader.dataset.x[0].cpu().numpy()\n",
    "real_row = np.append(real_row, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Time        V1        V2        V3         V4        V5        V6  \\\n",
       "0 -0.196971 -7.667089  5.699276 -10.15090  10.077229 -7.307253 -2.589641   \n",
       "1  0.910404 -5.839191  7.151532 -12.81676   7.031115 -9.651272 -2.938427   \n",
       "\n",
       "          V7        V8        V9  ...       V21       V22       V23      V24  \\\n",
       "0  -9.824335  3.019747 -7.658296  ...  1.073921  0.034662  0.247951  0.00464   \n",
       "1 -11.543207  4.843626 -3.494276  ...  2.462056  1.054865  0.530481  0.47267   \n",
       "\n",
       "        V25       V26       V27       V28    Amount  Class  \n",
       "0 -0.037674  0.597619  0.763070 -0.609457 -0.377716    1.0  \n",
       "1 -0.275998  0.282435  0.104886  0.254417  0.910404    1.0  \n",
       "\n",
       "[2 rows x 31 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.196971</td>\n      <td>-7.667089</td>\n      <td>5.699276</td>\n      <td>-10.15090</td>\n      <td>10.077229</td>\n      <td>-7.307253</td>\n      <td>-2.589641</td>\n      <td>-9.824335</td>\n      <td>3.019747</td>\n      <td>-7.658296</td>\n      <td>...</td>\n      <td>1.073921</td>\n      <td>0.034662</td>\n      <td>0.247951</td>\n      <td>0.00464</td>\n      <td>-0.037674</td>\n      <td>0.597619</td>\n      <td>0.763070</td>\n      <td>-0.609457</td>\n      <td>-0.377716</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.910404</td>\n      <td>-5.839191</td>\n      <td>7.151532</td>\n      <td>-12.81676</td>\n      <td>7.031115</td>\n      <td>-9.651272</td>\n      <td>-2.938427</td>\n      <td>-11.543207</td>\n      <td>4.843626</td>\n      <td>-3.494276</td>\n      <td>...</td>\n      <td>2.462056</td>\n      <td>1.054865</td>\n      <td>0.530481</td>\n      <td>0.47267</td>\n      <td>-0.275998</td>\n      <td>0.282435</td>\n      <td>0.104886</td>\n      <td>0.254417</td>\n      <td>0.910404</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 290
    }
   ],
   "source": [
    "df = pd.DataFrame(np.stack((recon_row, real_row)), columns = cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = torch.exp(logvar/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([0.0001, 0.0163, 0.0400]), tensor([0.9976, 0.0370, 0.0381]))"
      ]
     },
     "metadata": {},
     "execution_count": 294
    }
   ],
   "source": [
    "mu.mean(axis=0), sigma.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample z from q\n",
    "no_samples = 20\n",
    "q = torch.distributions.Normal(mu.mean(axis=0), sigma.mean(axis=0))\n",
    "z = q.rsample(sample_shape=torch.Size([no_samples]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model.decode(z).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Time         V1        V2        V3        V4         V5        V6  \\\n",
       "0 -1.014143   1.505616 -4.616234  7.718655 -0.977422   8.594662 -3.198405   \n",
       "1 -1.810440 -13.005595  1.212420  5.370727  2.069537  -1.141557 -3.816671   \n",
       "2 -1.152523  12.006341 -3.014931  4.485871 -1.155190  10.059814 -3.355832   \n",
       "3  0.228914  -5.935965 -1.644437 -6.354884  7.788726  -0.055751 -1.726003   \n",
       "4  0.180823  -3.444491  4.722339 -4.571048  4.998073  -4.543203 -0.816252   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0 -6.944025 -5.043085  2.561653  ...  1.094700  0.510489 -1.254657 -0.085117   \n",
       "1 -6.958980  4.140651 -1.208175  ...  0.902933 -0.573067  1.209823  0.543091   \n",
       "2 -8.342437 -8.336978  2.741910  ... -0.101801  1.417866 -2.335097  0.034988   \n",
       "3  0.577209  1.638260 -5.880371  ... -5.350942  2.994604 -0.079382 -1.020990   \n",
       "4 -5.482205  3.643872 -4.685173  ... -1.748235  1.525022  0.258438 -0.465014   \n",
       "\n",
       "        V25       V26       V27       V28      Amount  Class  \n",
       "0  0.283567 -0.268765  3.025049  0.929408  -79.125496      1  \n",
       "1  0.666637 -0.524895  0.204588 -0.074243 -380.632935      1  \n",
       "2 -0.466923 -0.012957  2.653872  1.081970 -163.960175      1  \n",
       "3 -0.090167  0.395981 -1.590370 -1.090804    9.417862      1  \n",
       "4  0.064509  0.277528  1.127516  0.161839  171.483337      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.014143</td>\n      <td>1.505616</td>\n      <td>-4.616234</td>\n      <td>7.718655</td>\n      <td>-0.977422</td>\n      <td>8.594662</td>\n      <td>-3.198405</td>\n      <td>-6.944025</td>\n      <td>-5.043085</td>\n      <td>2.561653</td>\n      <td>...</td>\n      <td>1.094700</td>\n      <td>0.510489</td>\n      <td>-1.254657</td>\n      <td>-0.085117</td>\n      <td>0.283567</td>\n      <td>-0.268765</td>\n      <td>3.025049</td>\n      <td>0.929408</td>\n      <td>-79.125496</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.810440</td>\n      <td>-13.005595</td>\n      <td>1.212420</td>\n      <td>5.370727</td>\n      <td>2.069537</td>\n      <td>-1.141557</td>\n      <td>-3.816671</td>\n      <td>-6.958980</td>\n      <td>4.140651</td>\n      <td>-1.208175</td>\n      <td>...</td>\n      <td>0.902933</td>\n      <td>-0.573067</td>\n      <td>1.209823</td>\n      <td>0.543091</td>\n      <td>0.666637</td>\n      <td>-0.524895</td>\n      <td>0.204588</td>\n      <td>-0.074243</td>\n      <td>-380.632935</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.152523</td>\n      <td>12.006341</td>\n      <td>-3.014931</td>\n      <td>4.485871</td>\n      <td>-1.155190</td>\n      <td>10.059814</td>\n      <td>-3.355832</td>\n      <td>-8.342437</td>\n      <td>-8.336978</td>\n      <td>2.741910</td>\n      <td>...</td>\n      <td>-0.101801</td>\n      <td>1.417866</td>\n      <td>-2.335097</td>\n      <td>0.034988</td>\n      <td>-0.466923</td>\n      <td>-0.012957</td>\n      <td>2.653872</td>\n      <td>1.081970</td>\n      <td>-163.960175</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.228914</td>\n      <td>-5.935965</td>\n      <td>-1.644437</td>\n      <td>-6.354884</td>\n      <td>7.788726</td>\n      <td>-0.055751</td>\n      <td>-1.726003</td>\n      <td>0.577209</td>\n      <td>1.638260</td>\n      <td>-5.880371</td>\n      <td>...</td>\n      <td>-5.350942</td>\n      <td>2.994604</td>\n      <td>-0.079382</td>\n      <td>-1.020990</td>\n      <td>-0.090167</td>\n      <td>0.395981</td>\n      <td>-1.590370</td>\n      <td>-1.090804</td>\n      <td>9.417862</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.180823</td>\n      <td>-3.444491</td>\n      <td>4.722339</td>\n      <td>-4.571048</td>\n      <td>4.998073</td>\n      <td>-4.543203</td>\n      <td>-0.816252</td>\n      <td>-5.482205</td>\n      <td>3.643872</td>\n      <td>-4.685173</td>\n      <td>...</td>\n      <td>-1.748235</td>\n      <td>1.525022</td>\n      <td>0.258438</td>\n      <td>-0.465014</td>\n      <td>0.064509</td>\n      <td>0.277528</td>\n      <td>1.127516</td>\n      <td>0.161839</td>\n      <td>171.483337</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 324
    }
   ],
   "source": [
    "df_fake = pd.DataFrame(pred)\n",
    "df_fake['Class']=1\n",
    "df_fake.columns = cols\n",
    "df_fake['Class'] = np.round(df_fake['Class']).astype(int)\n",
    "df_fake['Time'] = (df_fake['Time']*std_time)+mean_time\n",
    "df_fake['Amount'] = (df_fake['Amount']*std_amount)+mean_amount\n",
    "df_fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "121.77293"
      ]
     },
     "metadata": {},
     "execution_count": 325
    }
   ],
   "source": [
    "df_fake['Amount'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Class\n",
       "0     88.291022\n",
       "1    122.211321\n",
       "Name: Amount, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 338
    }
   ],
   "source": [
    "df.groupby('Class').mean()['Amount']"
   ]
  },
  {
   "source": [
    "Use fake data for oversampling in RandomForest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    199029\n",
       "1       335\n",
       "Name: Class, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 344
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "source": [
    "So let's build about 190.000 fake fraud detection cases:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_samples = 190_000\n",
    "q = torch.distributions.Normal(mu.mean(axis=0), sigma.mean(axis=0))\n",
    "z = q.rsample(sample_shape=torch.Size([no_samples]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model.decode(z).cpu().numpy()"
   ]
  },
  {
   "source": [
    "Concat to our X_train:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(389364, 30)"
      ]
     },
     "metadata": {},
     "execution_count": 365
    }
   ],
   "source": [
    "X_train_augmented = np.vstack((X_train.values, pred))\n",
    "y_train_augmented = np.append(y_train.values, np.repeat(1,no_samples))\n",
    "X_train_augmented.shape"
   ]
  },
  {
   "source": [
    "We now have roughly as many fraud cases as we have non-fraud cases. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Train RandomForest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_aug(xs, y, n_estimators=40, max_samples=500,\n",
    "       max_features=0.5, min_samples_leaf=5, **kwargs):\n",
    "    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n",
    "        max_samples=max_samples, max_features=max_features,\n",
    "        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[84963,   323],\n",
       "       [   30,   127]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 412
    }
   ],
   "source": [
    "m_aug = rf_aug(X_train_augmented, y_train_augmented)\n",
    "confusion_matrix(y_test, np.round(m_aug.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[85278,     8],\n",
       "       [  118,    39]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 413
    }
   ],
   "source": [
    "confusion_matrix(y_test, np.round(m.predict(X_test)))"
   ]
  },
  {
   "source": [
    "Look at that! We managed to find 127 out of 157! "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}