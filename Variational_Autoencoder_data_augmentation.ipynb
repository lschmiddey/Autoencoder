{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('deeplearning_venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "9139ca13fc640d8623238ac4ed44beace8a76f86a07bab6efe75c2506e18783d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Variational Autoencoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## How to create fake tabular data to enhance machine learning algorithms"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To train deeplearning models the more data the better. When we're thinking of image data, the deeplearnig community thought about a lot of tricks how to enhance the model given a dataset of images: image enhancement. Meaning that by rotating, flipping, blurring etc the image we can create more input data and also improve our model. \n",
    "\n",
    "Hoever, when thinking about tabular data, only few of these techniques exist. In this notebook I want to show you how to create a variational autoencoder to make use of data enhancement. I will create fake data, which is sampled from the learned distribution of the underlying data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "execution_count": 216
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "source": [
    "### Define path to dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/wine.csv'"
   ]
  },
  {
   "source": [
    "## Dataset Overview"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Wine  Alcohol  Malic.acid   Ash   Acl   Mg  Phenols  Flavanoids  \\\n",
       "0     1    14.23        1.71  2.43  15.6  127     2.80        3.06   \n",
       "1     1    13.20        1.78  2.14  11.2  100     2.65        2.76   \n",
       "2     1    13.16        2.36  2.67  18.6  101     2.80        3.24   \n",
       "3     1    14.37        1.95  2.50  16.8  113     3.85        3.49   \n",
       "4     1    13.24        2.59  2.87  21.0  118     2.80        2.69   \n",
       "\n",
       "   Nonflavanoid.phenols  Proanth  Color.int   Hue    OD  Proline  \n",
       "0                  0.28     2.29       5.64  1.04  3.92     1065  \n",
       "1                  0.26     1.28       4.38  1.05  3.40     1050  \n",
       "2                  0.30     2.81       5.68  1.03  3.17     1185  \n",
       "3                  0.24     2.18       7.80  0.86  3.45     1480  \n",
       "4                  0.39     1.82       4.32  1.04  2.93      735  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Wine</th>\n      <th>Alcohol</th>\n      <th>Malic.acid</th>\n      <th>Ash</th>\n      <th>Acl</th>\n      <th>Mg</th>\n      <th>Phenols</th>\n      <th>Flavanoids</th>\n      <th>Nonflavanoid.phenols</th>\n      <th>Proanth</th>\n      <th>Color.int</th>\n      <th>Hue</th>\n      <th>OD</th>\n      <th>Proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 440
    }
   ],
   "source": [
    "df_base = pd.read_csv(DATA_PATH, sep=',')\n",
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_base.columns"
   ]
  },
  {
   "source": [
    "## Build Data Loader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_standardize_data(path):\n",
    "    # read in from csv\n",
    "    df = pd.read_csv(path, sep=',')\n",
    "    # replace nan with -99\n",
    "    df = df.fillna(-99)\n",
    "    df = df.values.reshape(-1, df.shape[1]).astype('float32')\n",
    "    # randomly split\n",
    "    X_train, X_test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    # standardize values\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)   \n",
    "    return X_train, X_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DataBuilder(Dataset):\n",
    "    def __init__(self, path, train=True):\n",
    "        self.X_train, self.X_test, self.standardizer = load_and_standardize_data(DATA_PATH)\n",
    "        if train:\n",
    "            self.x = torch.from_numpy(self.X_train)\n",
    "            self.len=self.x.shape[0]\n",
    "        else:\n",
    "            self.x = torch.from_numpy(self.X_test)\n",
    "            self.len=self.x.shape[0]\n",
    "        del self.X_train\n",
    "        del self.X_test \n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_set=DataBuilder(DATA_PATH, train=True)\n",
    "testdata_set=DataBuilder(DATA_PATH, train=False)\n",
    "\n",
    "trainloader=DataLoader(dataset=traindata_set,batch_size=1024)\n",
    "testloader=DataLoader(dataset=testdata_set,batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor)"
      ]
     },
     "metadata": {},
     "execution_count": 225
    }
   ],
   "source": [
    "type(trainloader.dataset.x), type(testloader.dataset.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([124, 14]), torch.Size([54, 14]))"
      ]
     },
     "metadata": {},
     "execution_count": 226
    }
   ],
   "source": [
    "trainloader.dataset.x.shape, testloader.dataset.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 1.3598,  0.6284,  1.0812,  ..., -0.6414, -1.0709, -0.5182],\n",
       "        [ 0.0628, -0.5409, -0.6130,  ...,  0.3465,  1.3308, -0.2151],\n",
       "        [ 0.0628, -0.7557, -1.2870,  ...,  0.4324, -0.3984,  0.0420],\n",
       "        ...,\n",
       "        [-1.2343,  1.6904, -0.4855,  ...,  1.0338,  0.5485,  2.6682],\n",
       "        [ 0.0628, -0.3261, -0.7952,  ...,  0.0029, -0.7415, -0.7983],\n",
       "        [ 0.0628, -0.7437,  0.0428,  ..., -0.6843,  1.0700, -0.9861]])"
      ]
     },
     "metadata": {},
     "execution_count": 227
    }
   ],
   "source": [
    "trainloader.dataset.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method StandardScaler.inverse_transform of StandardScaler()>"
      ]
     },
     "metadata": {},
     "execution_count": 228
    }
   ],
   "source": [
    "trainloader.dataset.standardizer.inverse_transform"
   ]
  },
  {
   "source": [
    "## Build model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self,D_in,H=50,H2=12,latent_dim=3):\n",
    "        \n",
    "        #Encoder\n",
    "        super(Autoencoder,self).__init__()\n",
    "        self.linear1=nn.Linear(D_in,H)\n",
    "        self.lin_bn1 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear2=nn.Linear(H,H2)\n",
    "        self.lin_bn2 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear3=nn.Linear(H2,H2)\n",
    "        self.lin_bn3 = nn.BatchNorm1d(num_features=H2)\n",
    "        \n",
    "#         # Latent vectors mu and sigma\n",
    "        self.fc1 = nn.Linear(H2, latent_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=latent_dim)\n",
    "        self.fc21 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "#         # Sampling vector\n",
    "        self.fc3 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc_bn3 = nn.BatchNorm1d(latent_dim)\n",
    "        self.fc4 = nn.Linear(latent_dim, H2)\n",
    "        self.fc_bn4 = nn.BatchNorm1d(H2)\n",
    "        \n",
    "#         # Decoder\n",
    "        self.linear4=nn.Linear(H2,H2)\n",
    "        self.lin_bn4 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear5=nn.Linear(H2,H)\n",
    "        self.lin_bn5 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear6=nn.Linear(H,D_in)\n",
    "        self.lin_bn6 = nn.BatchNorm1d(num_features=D_in)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def encode(self, x):\n",
    "        lin1 = self.relu(self.lin_bn1(self.linear1(x)))\n",
    "        lin2 = self.relu(self.lin_bn2(self.linear2(lin1)))\n",
    "        lin3 = self.relu(self.lin_bn3(self.linear3(lin2)))\n",
    "\n",
    "        fc1 = F.relu(self.bn1(self.fc1(lin3)))\n",
    "\n",
    "        r1 = self.fc21(fc1)\n",
    "        r2 = self.fc22(fc1)\n",
    "        \n",
    "        return r1, r2\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def decode(self, z):\n",
    "        fc3 = self.relu(self.fc_bn3(self.fc3(z)))\n",
    "        fc4 = self.relu(self.fc_bn4(self.fc4(fc3)))\n",
    "\n",
    "        lin4 = self.relu(self.lin_bn4(self.linear4(fc4)))\n",
    "        lin5 = self.relu(self.lin_bn5(self.linear5(lin4)))\n",
    "        return self.lin_bn6(self.linear6(lin5))\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        # self.decode(z) ist später recon_batch, mu ist mu und logvar ist logvar\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(customLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n",
    "    \n",
    "    # x_recon ist der im forward im Model erstellte recon_batch, x ist der originale x Batch, mu ist mu und logvar ist logvar \n",
    "    def forward(self, x_recon, x, mu, logvar):\n",
    "        loss_MSE = self.mse_loss(x_recon, x)\n",
    "        loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        return loss_MSE + loss_KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a module and applies the specified weight initialization\n",
    "def weights_init_uniform_rule(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # get the number of the inputs\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n)\n",
    "        m.weight.data.uniform_(-y, y)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "source": [
    "If you want to better understand the variational autoencoder technique, look [here](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73).\n",
    "\n",
    "For better understanding this AutoencoderClass, let me go briefly through it. This is a variational autoencoder (VAE) with two hidden layers, which (by default, but you can change this) 50 and then 12 activations. The latent factors are set to 3 (you can change that, too). So we're first exploding our initially 14 variables to 50 activations, then condensing it to 12, then to 3. From these 3 latent factors we then sample to recreate the original 14 values. We do that by inflating the 3 latent factors back to 12, then 50 and finally 14 activations (we decode the latent factors so to speak). With this reconstructed batch (recon_batch) we compare it with the original batch, computate our loss and adjust the weights and biases via our gradient (our optimizer here will be Adam). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = data_set.x.shape[1]\n",
    "H = 50\n",
    "H2 = 12\n",
    "model = Autoencoder(D_in, H, H2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mse = customLoss()"
   ]
  },
  {
   "source": [
    "## Train Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1500\n",
    "log_interval = 50\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(trainloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_mse(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    if epoch % 200 == 0:        \n",
    "        print('====> Epoch: {} Average training loss: {:.4f}'.format(\n",
    "            epoch, train_loss / len(trainloader.dataset)))\n",
    "        train_losses.append(train_loss / len(trainloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        for batch_idx, data in enumerate(testloader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_mse(recon_batch, data, mu, logvar)\n",
    "            test_loss += loss.item()\n",
    "            if epoch % 200 == 0:        \n",
    "                print('====> Epoch: {} Average test loss: {:.4f}'.format(\n",
    "                    epoch, test_loss / len(testloader.dataset)))\n",
    "            test_losses.append(test_loss / len(testloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====> Epoch: 200 Average training loss: 12.3501\n",
      "====> Epoch: 200 Average test loss: 11.7777\n",
      "====> Epoch: 400 Average training loss: 10.1168\n",
      "====> Epoch: 400 Average test loss: 8.9987\n",
      "====> Epoch: 600 Average training loss: 9.2956\n",
      "====> Epoch: 600 Average test loss: 9.3548\n",
      "====> Epoch: 800 Average training loss: 8.9570\n",
      "====> Epoch: 800 Average test loss: 8.9647\n",
      "====> Epoch: 1000 Average training loss: 8.6688\n",
      "====> Epoch: 1000 Average test loss: 8.5866\n",
      "====> Epoch: 1200 Average training loss: 8.3341\n",
      "====> Epoch: 1200 Average test loss: 8.8371\n",
      "====> Epoch: 1400 Average training loss: 8.4063\n",
      "====> Epoch: 1400 Average test loss: 8.7891\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "source": [
    "We we're able to reduce the training and test loss but quite a bit, let's have a look at how the fake results actually look like vs the real results:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(testloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = trainloader.dataset.standardizer\n",
    "recon_row = scaler.inverse_transform(recon_batch[0].cpu().numpy())\n",
    "real_row = scaler.inverse_transform(testloader.dataset.x[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Wine    Alcohol  Malic.acid       Ash        Acl          Mg   Phenols  \\\n",
       "0  1.002792  13.535107    2.010303  2.557292  18.198132  112.606842  2.737524   \n",
       "1  1.000000  13.640000    3.100000  2.560000  15.200000  116.000000  2.700000   \n",
       "\n",
       "   Flavanoids  Nonflavanoid.phenols   Proanth  Color.int       Hue        OD  \\\n",
       "0    2.807587              0.320866  1.738254   4.899318  1.078039  3.187276   \n",
       "1    3.030000              0.170000  1.660000   5.100000  0.960000  3.360000   \n",
       "\n",
       "       Proline  \n",
       "0  1013.391479  \n",
       "1   845.000000  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Wine</th>\n      <th>Alcohol</th>\n      <th>Malic.acid</th>\n      <th>Ash</th>\n      <th>Acl</th>\n      <th>Mg</th>\n      <th>Phenols</th>\n      <th>Flavanoids</th>\n      <th>Nonflavanoid.phenols</th>\n      <th>Proanth</th>\n      <th>Color.int</th>\n      <th>Hue</th>\n      <th>OD</th>\n      <th>Proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.002792</td>\n      <td>13.535107</td>\n      <td>2.010303</td>\n      <td>2.557292</td>\n      <td>18.198132</td>\n      <td>112.606842</td>\n      <td>2.737524</td>\n      <td>2.807587</td>\n      <td>0.320866</td>\n      <td>1.738254</td>\n      <td>4.899318</td>\n      <td>1.078039</td>\n      <td>3.187276</td>\n      <td>1013.391479</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.000000</td>\n      <td>13.640000</td>\n      <td>3.100000</td>\n      <td>2.560000</td>\n      <td>15.200000</td>\n      <td>116.000000</td>\n      <td>2.700000</td>\n      <td>3.030000</td>\n      <td>0.170000</td>\n      <td>1.660000</td>\n      <td>5.100000</td>\n      <td>0.960000</td>\n      <td>3.360000</td>\n      <td>845.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 246
    }
   ],
   "source": [
    "df = pd.DataFrame(np.stack((recon_row, real_row)), columns = cols)\n",
    "df"
   ]
  },
  {
   "source": [
    "Not to bad right (the first row is the reconstructed row, the second one the real row from the data)? However, what we want is to built this row not with the real input so to speak, since right now we were giving the model the complete rows with their 14 columns, condensed it to 3 input parameters, just to blow it up again to the corresponding 14 columns. What I want to do is to create these 14 rows by giving the model 3 latent factors as input. Let's have a look at these latent variables. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = torch.exp(logvar/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([-0.9960, -0.8502, -0.0043]), tensor([0.2555, 0.4801, 0.9888]))"
      ]
     },
     "metadata": {},
     "execution_count": 256
    }
   ],
   "source": [
    "mu[1], sigma[1]"
   ]
  },
  {
   "source": [
    "Mu represents the mean for each of our latent factor values, logvar the log of the standard deviation. Each of these have a distribution by itself. We have 54 cases in our test data, so we have 3x54 different mu and logvar. We can have a look at the distribution of each of the 3 latent variables: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([-0.0088,  0.0051,  0.0044]), tensor([0.4514, 0.3897, 0.9986]))"
      ]
     },
     "metadata": {},
     "execution_count": 257
    }
   ],
   "source": [
    "mu.mean(axis=0), sigma.mean(axis=0)"
   ]
  },
  {
   "source": [
    "All of the latent variables have a mean around zero, but the last latent factor has a wider standard deviation. So when we sample values from each of these latent variables, the last value will vary much more then the other two. I assume a normal distribution for all the latent factors."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample z from q\n",
    "no_samples = 20\n",
    "q = torch.distributions.Normal(mu.mean(axis=0), sigma.mean(axis=0))\n",
    "z = q.rsample(sample_shape=torch.Size([no_samples]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([20, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 406
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.5283,  0.4519,  0.6792],\n",
       "        [ 0.3664, -0.5569, -0.1531],\n",
       "        [-0.5802,  0.4394,  1.8406],\n",
       "        [-1.0136, -0.4239,  0.4524],\n",
       "        [-0.0605,  0.3913,  0.8030]])"
      ]
     },
     "metadata": {},
     "execution_count": 446
    }
   ],
   "source": [
    "z[:5]"
   ]
  },
  {
   "source": [
    "With these three latent factors we can now start and create fake data for our dataset and see how it looks like:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model.decode(z).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-0.24290268, -0.6087041 , -0.44325534, -0.7158908 , -0.15065292,\n",
       "       -0.47845733,  0.26319185,  0.23732403, -0.22809544,  0.12187037,\n",
       "       -0.8295655 ,  0.44908378,  0.6173717 , -0.55648965], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 409
    }
   ],
   "source": [
    "pred[1]"
   ]
  },
  {
   "source": [
    "## Create fake data from Autoencoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(20, 14)"
      ]
     },
     "metadata": {},
     "execution_count": 420
    }
   ],
   "source": [
    "fake_data = scaler.inverse_transform(pred)\n",
    "fake_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Wine    Alcohol  Malic.acid       Ash        Acl          Mg   Phenols  \\\n",
       "0     3  13.350755    3.817283  2.425754  21.229387   98.816788  1.682916   \n",
       "1     2  12.453159    1.916350  2.172731  18.977226   93.556114  2.444676   \n",
       "2     2  12.735057    2.404566  2.447556  20.400013  105.475235  1.937112   \n",
       "3     1  14.664644    1.517465  2.269279  12.428186   88.851791  3.354010   \n",
       "4     3  13.160161    3.359397  2.415784  21.050211   99.859154  1.662516   \n",
       "5     2  12.453159    1.916350  2.172731  18.977226   93.556114  2.444676   \n",
       "6     2  12.520310    2.522696  2.375254  20.435560   92.619812  1.838333   \n",
       "7     3  12.877177    2.746192  2.395865  20.154610   97.263092  1.744550   \n",
       "8     2  12.679532    2.344776  2.331834  19.901327   97.031586  1.857117   \n",
       "9     2  13.062141    2.719065  2.461590  19.947014  103.352890  2.070540   \n",
       "\n",
       "   Flavanoids  Nonflavanoid.phenols   Proanth  Color.int       Hue        OD  \\\n",
       "0    0.910786              0.450081  1.245882   8.242197  0.667928  1.705379   \n",
       "1    2.246270              0.335432  1.663583   3.166457  1.063876  3.050176   \n",
       "2    1.657119              0.385740  1.452577   4.242754  0.928397  2.467263   \n",
       "3    3.997237              0.265253  2.586414   7.366968  1.275564  3.170231   \n",
       "4    0.929189              0.427978  1.135361   7.101127  0.708510  1.732820   \n",
       "5    2.246270              0.335432  1.663583   3.166457  1.063876  3.050176   \n",
       "6    1.361269              0.470815  1.221076   4.518130  0.906680  2.146883   \n",
       "7    1.187050              0.464942  1.160733   5.619783  0.836708  1.871472   \n",
       "8    1.495742              0.461352  1.239715   4.668478  0.934352  2.094139   \n",
       "9    1.566055              0.380154  1.293219   5.675068  0.852832  2.128047   \n",
       "\n",
       "       Proline  \n",
       "0   636.650818  \n",
       "1   568.385925  \n",
       "2   680.271545  \n",
       "3  1516.662720  \n",
       "4   640.412231  \n",
       "5   568.385925  \n",
       "6   583.079102  \n",
       "7   665.485718  \n",
       "8   680.778809  \n",
       "9   778.582825  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Wine</th>\n      <th>Alcohol</th>\n      <th>Malic.acid</th>\n      <th>Ash</th>\n      <th>Acl</th>\n      <th>Mg</th>\n      <th>Phenols</th>\n      <th>Flavanoids</th>\n      <th>Nonflavanoid.phenols</th>\n      <th>Proanth</th>\n      <th>Color.int</th>\n      <th>Hue</th>\n      <th>OD</th>\n      <th>Proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>13.350755</td>\n      <td>3.817283</td>\n      <td>2.425754</td>\n      <td>21.229387</td>\n      <td>98.816788</td>\n      <td>1.682916</td>\n      <td>0.910786</td>\n      <td>0.450081</td>\n      <td>1.245882</td>\n      <td>8.242197</td>\n      <td>0.667928</td>\n      <td>1.705379</td>\n      <td>636.650818</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>12.453159</td>\n      <td>1.916350</td>\n      <td>2.172731</td>\n      <td>18.977226</td>\n      <td>93.556114</td>\n      <td>2.444676</td>\n      <td>2.246270</td>\n      <td>0.335432</td>\n      <td>1.663583</td>\n      <td>3.166457</td>\n      <td>1.063876</td>\n      <td>3.050176</td>\n      <td>568.385925</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>12.735057</td>\n      <td>2.404566</td>\n      <td>2.447556</td>\n      <td>20.400013</td>\n      <td>105.475235</td>\n      <td>1.937112</td>\n      <td>1.657119</td>\n      <td>0.385740</td>\n      <td>1.452577</td>\n      <td>4.242754</td>\n      <td>0.928397</td>\n      <td>2.467263</td>\n      <td>680.271545</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>14.664644</td>\n      <td>1.517465</td>\n      <td>2.269279</td>\n      <td>12.428186</td>\n      <td>88.851791</td>\n      <td>3.354010</td>\n      <td>3.997237</td>\n      <td>0.265253</td>\n      <td>2.586414</td>\n      <td>7.366968</td>\n      <td>1.275564</td>\n      <td>3.170231</td>\n      <td>1516.662720</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>13.160161</td>\n      <td>3.359397</td>\n      <td>2.415784</td>\n      <td>21.050211</td>\n      <td>99.859154</td>\n      <td>1.662516</td>\n      <td>0.929189</td>\n      <td>0.427978</td>\n      <td>1.135361</td>\n      <td>7.101127</td>\n      <td>0.708510</td>\n      <td>1.732820</td>\n      <td>640.412231</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>12.453159</td>\n      <td>1.916350</td>\n      <td>2.172731</td>\n      <td>18.977226</td>\n      <td>93.556114</td>\n      <td>2.444676</td>\n      <td>2.246270</td>\n      <td>0.335432</td>\n      <td>1.663583</td>\n      <td>3.166457</td>\n      <td>1.063876</td>\n      <td>3.050176</td>\n      <td>568.385925</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2</td>\n      <td>12.520310</td>\n      <td>2.522696</td>\n      <td>2.375254</td>\n      <td>20.435560</td>\n      <td>92.619812</td>\n      <td>1.838333</td>\n      <td>1.361269</td>\n      <td>0.470815</td>\n      <td>1.221076</td>\n      <td>4.518130</td>\n      <td>0.906680</td>\n      <td>2.146883</td>\n      <td>583.079102</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>12.877177</td>\n      <td>2.746192</td>\n      <td>2.395865</td>\n      <td>20.154610</td>\n      <td>97.263092</td>\n      <td>1.744550</td>\n      <td>1.187050</td>\n      <td>0.464942</td>\n      <td>1.160733</td>\n      <td>5.619783</td>\n      <td>0.836708</td>\n      <td>1.871472</td>\n      <td>665.485718</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>12.679532</td>\n      <td>2.344776</td>\n      <td>2.331834</td>\n      <td>19.901327</td>\n      <td>97.031586</td>\n      <td>1.857117</td>\n      <td>1.495742</td>\n      <td>0.461352</td>\n      <td>1.239715</td>\n      <td>4.668478</td>\n      <td>0.934352</td>\n      <td>2.094139</td>\n      <td>680.778809</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>13.062141</td>\n      <td>2.719065</td>\n      <td>2.461590</td>\n      <td>19.947014</td>\n      <td>103.352890</td>\n      <td>2.070540</td>\n      <td>1.566055</td>\n      <td>0.380154</td>\n      <td>1.293219</td>\n      <td>5.675068</td>\n      <td>0.852832</td>\n      <td>2.128047</td>\n      <td>778.582825</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 439
    }
   ],
   "source": [
    "df_fake = pd.DataFrame(fake_data, columns = cols)\n",
    "df_fake['Wine'] = np.round(df_fake['Wine']).astype(int)\n",
    "df_fake['Wine'] = np.where(df_fake['Wine']<1, 1, df_fake['Wine'])\n",
    "df_fake.head(10)"
   ]
  },
  {
   "source": [
    "For comparison the real data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Wine  Alcohol  Malic.acid   Ash   Acl   Mg  Phenols  Flavanoids  \\\n",
       "1       1    13.20        1.78  2.14  11.2  100     2.65        2.76   \n",
       "35      1    13.48        1.81  2.41  20.5  100     2.70        2.98   \n",
       "114     2    12.08        1.39  2.50  22.5   84     2.56        2.29   \n",
       "149     3    13.08        3.90  2.36  21.5  113     1.41        1.39   \n",
       "158     3    14.34        1.68  2.70  25.0   98     2.80        1.31   \n",
       "9       1    13.86        1.35  2.27  16.0   98     2.98        3.15   \n",
       "90      2    12.08        1.83  2.32  18.5   81     1.60        1.50   \n",
       "47      1    13.90        1.68  2.12  16.0  101     3.10        3.39   \n",
       "10      1    14.10        2.16  2.30  18.0  105     2.95        3.32   \n",
       "31      1    13.58        1.66  2.36  19.1  106     2.86        3.19   \n",
       "\n",
       "     Nonflavanoid.phenols  Proanth  Color.int   Hue    OD  Proline  \n",
       "1                    0.26     1.28       4.38  1.05  3.40     1050  \n",
       "35                   0.26     1.86       5.10  1.04  3.47      920  \n",
       "114                  0.43     1.04       2.90  0.93  3.19      385  \n",
       "149                  0.34     1.14       9.40  0.57  1.33      550  \n",
       "158                  0.53     2.70      13.00  0.57  1.96      660  \n",
       "9                    0.22     1.85       7.22  1.01  3.55     1045  \n",
       "90                   0.52     1.64       2.40  1.08  2.27      480  \n",
       "47                   0.21     2.14       6.10  0.91  3.33      985  \n",
       "10                   0.22     2.38       5.75  1.25  3.17     1510  \n",
       "31                   0.22     1.95       6.90  1.09  2.88     1515  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Wine</th>\n      <th>Alcohol</th>\n      <th>Malic.acid</th>\n      <th>Ash</th>\n      <th>Acl</th>\n      <th>Mg</th>\n      <th>Phenols</th>\n      <th>Flavanoids</th>\n      <th>Nonflavanoid.phenols</th>\n      <th>Proanth</th>\n      <th>Color.int</th>\n      <th>Hue</th>\n      <th>OD</th>\n      <th>Proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>1</td>\n      <td>13.48</td>\n      <td>1.81</td>\n      <td>2.41</td>\n      <td>20.5</td>\n      <td>100</td>\n      <td>2.70</td>\n      <td>2.98</td>\n      <td>0.26</td>\n      <td>1.86</td>\n      <td>5.10</td>\n      <td>1.04</td>\n      <td>3.47</td>\n      <td>920</td>\n    </tr>\n    <tr>\n      <th>114</th>\n      <td>2</td>\n      <td>12.08</td>\n      <td>1.39</td>\n      <td>2.50</td>\n      <td>22.5</td>\n      <td>84</td>\n      <td>2.56</td>\n      <td>2.29</td>\n      <td>0.43</td>\n      <td>1.04</td>\n      <td>2.90</td>\n      <td>0.93</td>\n      <td>3.19</td>\n      <td>385</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>3</td>\n      <td>13.08</td>\n      <td>3.90</td>\n      <td>2.36</td>\n      <td>21.5</td>\n      <td>113</td>\n      <td>1.41</td>\n      <td>1.39</td>\n      <td>0.34</td>\n      <td>1.14</td>\n      <td>9.40</td>\n      <td>0.57</td>\n      <td>1.33</td>\n      <td>550</td>\n    </tr>\n    <tr>\n      <th>158</th>\n      <td>3</td>\n      <td>14.34</td>\n      <td>1.68</td>\n      <td>2.70</td>\n      <td>25.0</td>\n      <td>98</td>\n      <td>2.80</td>\n      <td>1.31</td>\n      <td>0.53</td>\n      <td>2.70</td>\n      <td>13.00</td>\n      <td>0.57</td>\n      <td>1.96</td>\n      <td>660</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>13.86</td>\n      <td>1.35</td>\n      <td>2.27</td>\n      <td>16.0</td>\n      <td>98</td>\n      <td>2.98</td>\n      <td>3.15</td>\n      <td>0.22</td>\n      <td>1.85</td>\n      <td>7.22</td>\n      <td>1.01</td>\n      <td>3.55</td>\n      <td>1045</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>2</td>\n      <td>12.08</td>\n      <td>1.83</td>\n      <td>2.32</td>\n      <td>18.5</td>\n      <td>81</td>\n      <td>1.60</td>\n      <td>1.50</td>\n      <td>0.52</td>\n      <td>1.64</td>\n      <td>2.40</td>\n      <td>1.08</td>\n      <td>2.27</td>\n      <td>480</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>1</td>\n      <td>13.90</td>\n      <td>1.68</td>\n      <td>2.12</td>\n      <td>16.0</td>\n      <td>101</td>\n      <td>3.10</td>\n      <td>3.39</td>\n      <td>0.21</td>\n      <td>2.14</td>\n      <td>6.10</td>\n      <td>0.91</td>\n      <td>3.33</td>\n      <td>985</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1</td>\n      <td>14.10</td>\n      <td>2.16</td>\n      <td>2.30</td>\n      <td>18.0</td>\n      <td>105</td>\n      <td>2.95</td>\n      <td>3.32</td>\n      <td>0.22</td>\n      <td>2.38</td>\n      <td>5.75</td>\n      <td>1.25</td>\n      <td>3.17</td>\n      <td>1510</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>1</td>\n      <td>13.58</td>\n      <td>1.66</td>\n      <td>2.36</td>\n      <td>19.1</td>\n      <td>106</td>\n      <td>2.86</td>\n      <td>3.19</td>\n      <td>0.22</td>\n      <td>1.95</td>\n      <td>6.90</td>\n      <td>1.09</td>\n      <td>2.88</td>\n      <td>1515</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 444
    }
   ],
   "source": [
    "df_base.sample(10)"
   ]
  },
  {
   "source": [
    "## Compare variables grouped by Wine"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        Alcohol  Malic.acid       Ash        Acl          Mg   Phenols  \\\n",
       "Wine                                                                     \n",
       "1     13.744746    2.010678  2.455593  17.037288  106.338983  2.840169   \n",
       "2     12.278732    1.932676  2.244789  20.238028   94.549296  2.258873   \n",
       "3     13.153750    3.333750  2.437083  21.416667   99.312500  1.678750   \n",
       "\n",
       "      Flavanoids  Nonflavanoid.phenols   Proanth  Color.int       Hue  \\\n",
       "Wine                                                                    \n",
       "1       2.982373              0.290000  1.899322   5.528305  1.062034   \n",
       "2       2.080845              0.363662  1.630282   3.086620  1.056282   \n",
       "3       0.781458              0.447500  1.153542   7.396250  0.682708   \n",
       "\n",
       "            OD      Proline  \n",
       "Wine                         \n",
       "1     3.157797  1115.711864  \n",
       "2     2.785352   519.507042  \n",
       "3     1.683542   629.895833  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Alcohol</th>\n      <th>Malic.acid</th>\n      <th>Ash</th>\n      <th>Acl</th>\n      <th>Mg</th>\n      <th>Phenols</th>\n      <th>Flavanoids</th>\n      <th>Nonflavanoid.phenols</th>\n      <th>Proanth</th>\n      <th>Color.int</th>\n      <th>Hue</th>\n      <th>OD</th>\n      <th>Proline</th>\n    </tr>\n    <tr>\n      <th>Wine</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>13.744746</td>\n      <td>2.010678</td>\n      <td>2.455593</td>\n      <td>17.037288</td>\n      <td>106.338983</td>\n      <td>2.840169</td>\n      <td>2.982373</td>\n      <td>0.290000</td>\n      <td>1.899322</td>\n      <td>5.528305</td>\n      <td>1.062034</td>\n      <td>3.157797</td>\n      <td>1115.711864</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12.278732</td>\n      <td>1.932676</td>\n      <td>2.244789</td>\n      <td>20.238028</td>\n      <td>94.549296</td>\n      <td>2.258873</td>\n      <td>2.080845</td>\n      <td>0.363662</td>\n      <td>1.630282</td>\n      <td>3.086620</td>\n      <td>1.056282</td>\n      <td>2.785352</td>\n      <td>519.507042</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13.153750</td>\n      <td>3.333750</td>\n      <td>2.437083</td>\n      <td>21.416667</td>\n      <td>99.312500</td>\n      <td>1.678750</td>\n      <td>0.781458</td>\n      <td>0.447500</td>\n      <td>1.153542</td>\n      <td>7.396250</td>\n      <td>0.682708</td>\n      <td>1.683542</td>\n      <td>629.895833</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 443
    }
   ],
   "source": [
    "df_base.groupby('Wine').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        Alcohol  Malic.acid       Ash        Acl          Mg   Phenols  \\\n",
       "Wine                                                                     \n",
       "1     13.812141    1.814212  2.482638  17.172688  107.468864  3.062387   \n",
       "2     12.560544    2.157595  2.301805  19.696327   99.324005  2.254415   \n",
       "3     13.170316    3.413856  2.416369  20.929930   99.028229  1.683604   \n",
       "\n",
       "      Flavanoids  Nonflavanoid.phenols   Proanth  Color.int       Hue  \\\n",
       "Wine                                                                    \n",
       "1       3.344664              0.259955  2.162966   5.331643  1.147217   \n",
       "2       1.995140              0.366076  1.575015   3.791955  1.000527   \n",
       "3       0.964315              0.443444  1.176529   7.288512  0.718357   \n",
       "\n",
       "            OD      Proline  \n",
       "Wine                         \n",
       "1     3.280716  1148.031372  \n",
       "2     2.741598   629.895203  \n",
       "3     1.745200   644.870056  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Alcohol</th>\n      <th>Malic.acid</th>\n      <th>Ash</th>\n      <th>Acl</th>\n      <th>Mg</th>\n      <th>Phenols</th>\n      <th>Flavanoids</th>\n      <th>Nonflavanoid.phenols</th>\n      <th>Proanth</th>\n      <th>Color.int</th>\n      <th>Hue</th>\n      <th>OD</th>\n      <th>Proline</th>\n    </tr>\n    <tr>\n      <th>Wine</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>13.812141</td>\n      <td>1.814212</td>\n      <td>2.482638</td>\n      <td>17.172688</td>\n      <td>107.468864</td>\n      <td>3.062387</td>\n      <td>3.344664</td>\n      <td>0.259955</td>\n      <td>2.162966</td>\n      <td>5.331643</td>\n      <td>1.147217</td>\n      <td>3.280716</td>\n      <td>1148.031372</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12.560544</td>\n      <td>2.157595</td>\n      <td>2.301805</td>\n      <td>19.696327</td>\n      <td>99.324005</td>\n      <td>2.254415</td>\n      <td>1.995140</td>\n      <td>0.366076</td>\n      <td>1.575015</td>\n      <td>3.791955</td>\n      <td>1.000527</td>\n      <td>2.741598</td>\n      <td>629.895203</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13.170316</td>\n      <td>3.413856</td>\n      <td>2.416369</td>\n      <td>20.929930</td>\n      <td>99.028229</td>\n      <td>1.683604</td>\n      <td>0.964315</td>\n      <td>0.443444</td>\n      <td>1.176529</td>\n      <td>7.288512</td>\n      <td>0.718357</td>\n      <td>1.745200</td>\n      <td>644.870056</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 445
    }
   ],
   "source": [
    "df_fake.groupby('Wine').mean()"
   ]
  },
  {
   "source": [
    "That looks pretty convincing if you ask me. \n",
    "\n",
    "To sum up, we've built a variational autoencoder, which we trained on our trainingset. We checked whether our loss kept on improving based on the testset, which the autoencoder never saw for generating fake data. We then calculated the mean and standard deviation from our latent factors given the test data. We've then sampled from this distribution to feed it back into our decoder to create some fake data. With this approach I am now able to create as much fake data derived from the underlying distribution as a want. And I think the results look promising. \n",
    "\n",
    "You can take this approach to for example create data from under-represented in highly skewed datasets instead of just weighting them higher. The re-weighting approach might cause the algorithm to find relations where there are none, only because a few then overrepresented data points share this relation by random. With the shown approach, the learned distribution would take into account the high variance these features have and therefore will hopefully help the algorithm to not draw these false conclusions.\n",
    "\n",
    "Stay tuned for the next blogpost, where I will show the shown approach in exactly this use case."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}